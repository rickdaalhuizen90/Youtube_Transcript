 The word allocate literally means to split up and distribute parts of a limited resource.
 Governments, for example, have a tax budget that they split up and allocate to different
 things like healthcare and education.
 And in exactly the same way, your computer has a limited amount of memory that can be
 allocated to different programs or applications.
 And when it comes to government spending, it's not like there's one person who decides
 how every single dollar should be spent.
 At the top level, governments have someone like a treasurer who decides in very broad
 general terms how the budget should be divided up.
 They might, for example, allocate 10% of the total budget to defense, 30% to healthcare,
 and 20% to education.
 But just like the treasurer, the person in charge of education doesn't spend every
 dollar that they've been given on their own.
 There are many more layers of people who allocate progressively smaller and more specific
 budgets until we get all the way down to a teacher, buying office
 supplies or resources for their students. If you wanted, you could refer to each one
 of these people in this hierarchy as an allocator. Because in simple terms, all an allocator does
 is take a resource that's been given to them, split it up, and then allocate it further.
 This is exactly how memory allocation works on a computer. The operating system is just
 like the treasurer sitting right at the top of the hierarchy, allocating memory in bulk
 for programs to use. Then inside programs themselves, there are often several more
 layers of allocators that are choosing how to further divide up the memory that's been allocated
 to the program. Now that's all well and good, but so far we've only talked about giving out
 resources, which is called allocation. What about deallocation, which is taking those
 resources back again? In our government budget example, if we cancel or deallocate two budget
 items that both cost $10,000, we now have a combined total of $20,000 to spend on something
 else. And that's because in allocation
 of money only has one property, which is the quantity or how much money it is. Doesn't matter
 where the money came from or what it was previously allocated to, $20,000 is $20,000 in budgetary
 terms. An allocation of computer memory, on the other hand, has two properties. It does have a
 quantity or a size, like 50 kilobytes or 1 gigabyte, but an allocation of memory also has a position,
 otherwise known as an address. The same quantity of memory can be allocated in different places.
 And it might not seem so obvious on the surface, but that second property, the fact that memory
 has an address, means that deallocating memory is much more complicated than deallocating money.
 And to illustrate what I mean, imagine we're trying to write an allocator for an operating
 system at the top level to hand out those bulk pieces of memory to applications.
 So a program starts up, and it makes a request for two gigabytes of memory.
 Now, if we want to fulfill that request, we'll need some kind of allocation.
 to divide up 2GB of memory and allocate it to the application.
 Let's start with the simplest possible allocator.
 It's called a linear allocator.
 And to build one, you only need one single number variable,
 which is the address in memory of where the next allocation is going to go.
 So from a blank slate,
 if our first program is requesting 2GB of memory,
 the next address starts off at 0.
 So we allocate 2GB of memory at address 0,
 and then increment the next address by the size of the allocation.
 So in this case, the next address moves up from 0 to 2GB.
 Then we have a second program start up and request 5GB.
 So the next address moves up to 7.
 Now, if our second program exits, we can reclaim that memory
 and reuse it by subtracting instead of adding to the next address.
 So we subtract the 5GB that the second program was using,
 And now we're back at two.
 Great.
 But what happens if I'll-
 first program exits first.
 We can't rewind the next address to zero,
 because then as soon as we allocate more than two gigabytes,
 we'd end up overwriting the memory
 we allocated to our second program.
 So even though our first program has exited,
 we need to leave the next address at seven.
 So if a third program now requests two gigabytes,
 it's gonna be allocated at the end at seven,
 even though it could have fit in at zero.
 You can see where this is going.
 If any program exits other than the most recently started one,
 we end up with a hole in memory that we can't use.
 And the technical term for this type of phenomenon
 is fragmentation.
 On top of that, if we continue allocating like this,
 we're likely to just run out of memory completely at some point.
 Luckily, a linear allocator is not the only allocator in town.
 Over the decades, programmers have thought of a whole range
 of different types of allocators.
 In simplified terms, the way that a modern OS allocates memory to programs is
 by dividing up memory into lots and lots of small blocks.
 Then when a process asks for two gigabytes or for five gigabytes, instead of handing the
 process a single unbroken block, the OS hands over a number of smaller blocks that add
 up to the amount requested.
 Then when the process exits and the memory is deallocated, the OS marks all the blocks
 that were owned by that process as free.
 Then when a new process starts up and asks for memory, the OS reallocates those free
 blocks from the list, allowing memory to be reused.
 And you've probably already figured this out, but by dividing memory into small pieces,
 it means that the memory handed over to applications doesn't need to be altogether in one place.
 If I have two gigabytes of free blocks at the beginning, and three gigabytes of free
 blocks at the end, if a process starts up and requests five gigs, the OS can make up
 that number from two at the beginning and three at the end.
 We refer to memory that is all together.
 with sequentially increasing addresses as contiguous.
 So we can say that memory allocated by the OS is not guaranteed to be contiguous.
 There are of course some complexity and performance trade-offs that come with
 non-contiguous memory, but they're considered to be well worthwhile in order
 to prevent fragmentation and memory exhaustion. Now, it probably sounds like
 I'm saying that linear allocators are just bad, but that is not true at all.
 In fact, linear allocators are by far the simplest and fastest allocators for
 certain situations. As I mentioned before, with a linear allocator, you can
 only reclaim memory at the end of your allocations. But of course that's perfectly
 fine if your data structure or algorithm is guaranteed to only add or remove
 things from the very end of your memory. And if you know a little bit about
 data structures, you'll know that what I'm referring to is called a stack. And
 because of just how fundamental stack data structures are, if you're a
 programmer, a linear allocator is
 probably the one that you're using by far the most in your programs.
 Whenever you call a function, your program needs memory to store data that the function
 uses. You've got the arguments that get passed to the function, any local variables declared
 inside the function, and the return address of where the function's supposed to go back
 to when it's finished. These all need to be stored somewhere in memory. The chunk of
 memory that gets allocated to hold all of the data that a function uses is called a stack
 frame, and it's allocated using a linear allocator, which means that every new function
 call just sticks its stack frame onto the end of the previous one. And the reason that
 we can use a linear allocator to put stack frames into is because of a natural property
 of the way that functions work. Let me show you. If I have a main function as the very
 first entry point into my program, when the main function gets called, it's going to
 put the first stack frame into my linear allocator, and increment that offset for the
 just like the example we worked through before.
 Now, if I call another function called drawgraphics from inside my main function,
 the drawgraphics function is going to push its own stack frame onto the end of the linear allocator.
 And you can naturally see here, because of the way that the code in the main function
 is always going to execute in order from top to bottom.
 The drawgraphics function is always going to return and be deallocated
 before the main function does, which allows us to wind back the end of our linear allocator.
 If we call the second function, then it's free to override the memory that was allocated to drawgraphics,
 because we know that we're going to be finished with that function by the time the next function gets called.
 And then, of course, in the same way as the drawgraphics function,
 the autosave function also has to return before our main function ends,
 allowing us to safely wind back our linear allocator.
 There's a fundamental rule here,
 Whenever a function returns, it's guaranteed to be the very last stack frame in that linear
 allocator, so we can always wind back the next address, reclaim that memory, and not
 get any fragmentation.
 To look at this another way, we can visualize our function calls like a hierarchy or a tree.
 The main function is at the very top, and it calls out to subfunctions, which in turn
 call out to their own subfunctions.
 And because of the way code executes in order from top to bottom, a node in this function
 tree will never deallocate until all of its children have been deallocated.
 And as a result, it's a perfect fit for a linear allocator, or some people more directly
 call it in this case, a stack allocator.
 Just as a final tiny note, the more experienced programmers in the audience might have heard
 me say that code always executes from top to bottom and thought, well what about asynchronous
 Well, you're totally right.
 asynchronous functions require a more complex allocation strategy for managing their data,
 and that's just one of the reasons why async support in programming languages is so complex
 to implement. Alligators can sound kind of intimidating on the surface, but as we've seen,
 all an allocator really does is take memory that's been allocated to it, subdivide it further,
 and then hand it out to other things. It's actually very simple. And if you want to see
 an example of an allocator that's not just at the operating system or the programming language level,
 I have another video about memory arenas, which are a special type of allocator that you can
 implement yourself.
